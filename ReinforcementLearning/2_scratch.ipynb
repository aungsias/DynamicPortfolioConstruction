{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                MTCH        AZO        PEP        XOM        COF       RVTY  \\\n",
       " Date                                                                          \n",
       " 2005-01-04  6.262746  89.699997  30.812344  26.729958  62.349834  18.821409   \n",
       " 2005-01-05  6.213877  90.290001  30.848204  26.590258  61.728230  18.260733   \n",
       " 2005-01-06  6.196771  89.550003  31.069269  26.928753  62.501431  18.441881   \n",
       " 2005-01-07  6.257859  89.730003  31.338142  26.751440  62.129990  18.433252   \n",
       " 2005-01-10  6.270078  90.739998  32.120846  26.853527  62.395287  18.459126   \n",
       " \n",
       "                     GE   CDNS        FCX        EQR  ...   AZO_RET   PEP_RET  \\\n",
       " Date                                                 ...                       \n",
       " 2005-01-04  131.925140  13.30  11.094222  14.954946  ... -0.011638 -0.007150   \n",
       " 2005-01-05  131.122208  13.31  10.966949  14.475262  ...  0.006556  0.001163   \n",
       " 2005-01-06  132.180527  13.34  10.951435  14.475262  ... -0.008230  0.007141   \n",
       " 2005-01-07  131.377762  13.28  11.056971  14.411595  ...  0.002008  0.008617   \n",
       " 2005-01-10  131.049286  13.37  10.997992  14.326694  ...  0.011193  0.024669   \n",
       " \n",
       "              XOM_RET   COF_RET  RVTY_RET    GE_RET  CDNS_RET   FCX_RET  \\\n",
       " Date                                                                     \n",
       " 2005-01-04 -0.006811 -0.018669 -0.012298 -0.012098 -0.033275 -0.040307   \n",
       " 2005-01-05 -0.005240 -0.010020 -0.030242 -0.006105  0.000752 -0.011538   \n",
       " 2005-01-06  0.012650  0.012448  0.009871  0.008039  0.002251 -0.001416   \n",
       " 2005-01-07 -0.006606 -0.005961 -0.000468 -0.006092 -0.004508  0.009591   \n",
       " 2005-01-10  0.003809  0.004261  0.001403 -0.002503  0.006754 -0.005348   \n",
       " \n",
       "              EQR_RET   XEL_RET  \n",
       " Date                            \n",
       " 2005-01-04 -0.006225 -0.015097  \n",
       " 2005-01-05 -0.032601 -0.011331  \n",
       " 2005-01-06  0.000000 -0.001711  \n",
       " 2005-01-07 -0.004408 -0.005724  \n",
       " 2005-01-10 -0.005909  0.013683  \n",
       " \n",
       " [5 rows x 22 columns],\n",
       " (4732, 22))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(\"workflow/data/features.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "features.head(), features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4702, 30, 22)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookback = 30\n",
    "n_samples = len(features) - lookback\n",
    "n_features = len(features.columns)\n",
    "\n",
    "n_samples, lookback, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4702, 30, 22)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([features.iloc[i-lookback:i] for i in range(lookback, len(features))])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class TradeEnv(gym.Env):\n",
    "    def __init__(self, data, tc=0.002):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.n_assets = self.data.shape[-1] // 2\n",
    "        self.portfolio = np.zeros(self.n_assets)\n",
    "        self.current_step = 0\n",
    "        self.action_space = spaces.MultiDiscrete([3] * self.n_assets)\n",
    "        self.done = False\n",
    "        self.portfolio_returns = [0, ]\n",
    "        self.tc = tc\n",
    "    \n",
    "    def step(self, action_vector):\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        next_state = self.data[self.current_step]\n",
    "        total_price = sum(next_state[0][:self.n_assets])\n",
    "\n",
    "        step_return = 0  # Initialize step return\n",
    "        for i in range(self.n_assets):\n",
    "            action = action_vector[i]\n",
    "            returns = next_state[0][i + self.n_assets]\n",
    "            price_proportion = next_state[0][i] / total_price if total_price > 0 else 0\n",
    "\n",
    "            if action == 1:\n",
    "                step_return += (returns * price_proportion) - self.tc  # Transaction cost applied\n",
    "                self.portfolio[i] += 1\n",
    "            elif action == 2:\n",
    "                step_return -= (returns * price_proportion) + self.tc  # Transaction cost applied\n",
    "                self.portfolio[i] -= 1\n",
    "\n",
    "\n",
    "        self.portfolio_returns.append(step_return)  # Append step return to portfolio returns\n",
    "        volatility = np.std(self.portfolio_returns)\n",
    "        mean_portfolio_return = np.mean(self.portfolio_returns)\n",
    "        sharpe = mean_portfolio_return / volatility if volatility != 0 else 0  # Check for division by zero\n",
    "\n",
    "        if self.done:\n",
    "            self.current_step = 0\n",
    "            self.portfolio = np.zeros(self.n_assets)\n",
    "            self.portfolio_returns = [0, ]  # Reset to empty list\n",
    "\n",
    "        return next_state, sharpe, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.portfolio = np.zeros(self.n_assets)\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.portfolio_returns = [0, ]\n",
    "        return self.data[self.current_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class LSTMDQN(nn.Module):\n",
    "    def __init__(self, input_dim, n_assets, hidden_dim=64, n_layers=1):\n",
    "        super(LSTMDQN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, n_assets * 3)  # Three actions per asset: hold, buy, sell\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(1, x.size(0), 64).to(x.device)\n",
    "        c_0 = torch.zeros(1, x.size(0), 64).to(x.device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc1(out[:, -1, :])  # Use the last LSTM output only\n",
    "        out = self.fc2(out)\n",
    "        return out.view(out.size(0), -1, 3)  # Reshape to (batch_size, n_assets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your environment\n",
    "env = TradeEnv(X[:(len(X)//2)])  # Replace 'data' with your actual data array\n",
    "n_assets = env.n_assets\n",
    "\n",
    "# Initialize the DQN model\n",
    "input_dim = 22  # Number of features\n",
    "model = LSTMDQN(input_dim, n_assets)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dceb2e1de92443598f2180b4af6b664c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Episode Reward: -5144.673392375755\n",
      "Episode 2, Episode Reward: -5333.034206299664\n",
      "Episode 3, Episode Reward: -5389.174351476372\n",
      "Episode 4, Episode Reward: -5589.210034088686\n",
      "Episode 5, Episode Reward: -5463.713089841311\n",
      "Episode 6, Episode Reward: -5523.735496703524\n",
      "Episode 7, Episode Reward: -5636.696202021589\n",
      "Episode 8, Episode Reward: -5180.571737226846\n",
      "Episode 9, Episode Reward: -4449.08069387108\n",
      "Episode 10, Episode Reward: -4834.179559749112\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "n_episodes = 10\n",
    "lookback = 50  # Define your lookback window\n",
    "epsilon = 0.1  # For Îµ-greedy action selection\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    state = env.reset()\n",
    "    state = state[:lookback]  # Consider only 'lookback' recent states\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(3, size=n_assets)  # Random action\n",
    "        else:\n",
    "            action = torch.argmax(q_values, dim=2).cpu().numpy()[0]  # Greedy action\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state[:lookback]\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Q-learning update\n",
    "        # Q-learning update\n",
    "        # Q-learning update\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            max_next_q_values, _ = torch.max(next_q_values, dim=2)\n",
    "            target = (reward + 0.99 * max_next_q_values).detach()  # Detach target from computation graph\n",
    "\n",
    "                # Expanding dimensions of action_indices to match q_values\n",
    "        action_indices_expanded = torch.LongTensor(action).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        # Gathering chosen q_values based on actions\n",
    "        chosen_q_values = torch.gather(q_values, 2, action_indices_expanded).squeeze(2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(chosen_q_values, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode+1}, Episode Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d0385e54714180896a5049311a2987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aungs\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\aungs\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\aungs\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Test Episode 1, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan]\n",
      "[]\n",
      "Test Episode 2, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan]\n",
      "[]\n",
      "Test Episode 3, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 4, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 5, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 6, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 7, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 8, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 9, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "[]\n",
      "Test Episode 10, Episode Reward: -8033.0742275396415, Sharpe Ratio: nan\n",
      "[0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "Average Test Reward: -8033.074227539642, Average Test Sharpe Ratio: nan\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, env, n_episodes=10, lookback=50):\n",
    "    rewards = []\n",
    "    sharpe_ratios = [0, ]\n",
    "\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        state = env.reset()\n",
    "        state = state[:lookback]  # Consider only 'lookback' recent states\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = model(state_tensor)\n",
    "            action = torch.argmax(q_values, dim=2).cpu().numpy()[0]  # Greedy action\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = next_state[:lookback]\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        sharpe_ratio = episode_reward / np.std(env.portfolio_returns) if np.std(env.portfolio_returns) != 0 else 0\n",
    "        sharpe_ratios.append(sharpe_ratio)\n",
    "        rewards.append(episode_reward)\n",
    "        print(env.portfolio_returns)\n",
    "\n",
    "        print(f\"Test Episode {episode+1}, Episode Reward: {episode_reward}, Sharpe Ratio: {sharpe_ratio}\")\n",
    "        print(sharpe_ratios)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    avg_sharpe_ratio = np.mean(sharpe_ratios)\n",
    "    print(f\"Average Test Reward: {avg_reward}, Average Test Sharpe Ratio: {avg_sharpe_ratio}\")\n",
    "\n",
    "# Test the model after training\n",
    "test_model(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X22sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m rewards\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rewards' is not defined"
     ]
    }
   ],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
